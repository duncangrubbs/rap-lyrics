{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Rapper\n",
    "\n",
    "### _Generating rap lyrics with different AI and NLP techniques_\n",
    "\n",
    "We pull rap lyrics off of the Genius API, then feed them through a RNN, eventually asking the network to produce new text from a seed. Then we use n-Gram models to generate lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T18:33:15.670464Z",
     "start_time": "2019-10-14T18:33:15.662510Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Lyrics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T18:32:13.444095Z",
     "start_time": "2019-10-14T18:32:11.758979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for songs by DaBaby...\n",
      "\n",
      "Song 1: \"Suge\"\n",
      "Song 2: \"INTRO\"\n",
      "Song 3: \"BOP\"\n",
      "Song 4: \"Next Song\"\n",
      "Song 5: \"Goin Baby\"\n",
      "Song 6: \"VIBEZ\"\n",
      "Song 7: \"Baby Sitter\"\n",
      "Song 8: \"TOES\"\n",
      "Song 9: \"21\"\n",
      "Song 10: \"Today (Intro)\"\n",
      "Song 11: \"Walker Texas Ranger\"\n",
      "Song 12: \"Blank Blank\"\n",
      "Song 13: \"OFF THE RIP\"\n",
      "Song 14: \"Pull Up Music\"\n",
      "Song 15: \"XXL\"\n",
      "Song 16: \"4x\"\n",
      "Song 17: \"GOSPEL\"\n",
      "Song 18: \"POP STAR\"\n",
      "Song 19: \"RAW SHIT\"\n",
      "Song 20: \"Pony\"\n",
      "\n",
      "Reached user-specified song limit (20).\n",
      "Done. Found 20 songs.\n",
      "Wrote `Lyrics_DaBaby.json`\n"
     ]
    }
   ],
   "source": [
    "import lyricsgenius as genius\n",
    "# lol don't use my API key\n",
    "geniusCreds = 'Lw6NjXtbU7NndUFHRCcOX9FdLhPzVokLIt9c4LWzsTxM10wF7EICGtWSSso8Ohsq'\n",
    "# for more data, you can adjust the number of artists as well as the max number of songs\n",
    "artist_names = ['DaBaby', 'Drake', 'J. Cole', 'Travis Scott', 'Kendrick Lamar']\n",
    "\n",
    "api = genius.Genius(geniusCreds)\n",
    "\n",
    "for artist_name in artist_names:\n",
    "    artist = api.search_artist(artist_name, max_songs=20)\n",
    "    artist.save_lyrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data and Produce Basic Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the json as a dict\n",
    "text = \"\"\n",
    "for filename in os.listdir('./data/'):\n",
    "    with open('./data/' + filename) as json_data:\n",
    "        data = json.load(json_data)\n",
    "\n",
    "        for song in data['songs']:\n",
    "            text += song['lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Accuracy: 0.1700664496363421\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cnt = Counter()\n",
    "\n",
    "for char in text:\n",
    "    cnt[char] += 1\n",
    "\n",
    "most_com = cnt.most_common()[0]\n",
    "\n",
    "# if we were to run a baseline model, predicting every character as a space\n",
    "correct = 0\n",
    "for char in text:\n",
    "    if char == most_com[0]:\n",
    "        correct += 1\n",
    "        \n",
    "# print the accuracy\n",
    "print(f'Baseline Model Accuracy: {correct/len(text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Construction & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T21:10:08.952688Z",
     "start_time": "2019-10-14T21:09:02.996574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "25549/25549 [==============================] - 24s 927us/step - loss: 2.4127 - categorical_crossentropy: 2.4127 - accuracy: 0.3415\n",
      "Epoch 2/75\n",
      "25549/25549 [==============================] - 24s 928us/step - loss: 1.7506 - categorical_crossentropy: 1.7506 - accuracy: 0.4839\n",
      "Epoch 3/75\n",
      "25549/25549 [==============================] - 23s 914us/step - loss: 1.5949 - categorical_crossentropy: 1.5949 - accuracy: 0.5229\n",
      "Epoch 4/75\n",
      "25549/25549 [==============================] - 23s 887us/step - loss: 1.5253 - categorical_crossentropy: 1.5253 - accuracy: 0.5409\n",
      "Epoch 5/75\n",
      "25549/25549 [==============================] - 23s 888us/step - loss: 1.4826 - categorical_crossentropy: 1.4826 - accuracy: 0.5526\n",
      "Epoch 6/75\n",
      "25549/25549 [==============================] - 23s 884us/step - loss: 1.4547 - categorical_crossentropy: 1.4547 - accuracy: 0.5603\n",
      "Epoch 7/75\n",
      "25549/25549 [==============================] - 22s 876us/step - loss: 1.4332 - categorical_crossentropy: 1.4332 - accuracy: 0.5665\n",
      "Epoch 8/75\n",
      "25549/25549 [==============================] - 22s 875us/step - loss: 1.4167 - categorical_crossentropy: 1.4167 - accuracy: 0.5710\n",
      "Epoch 9/75\n",
      "25549/25549 [==============================] - 22s 873us/step - loss: 1.4033 - categorical_crossentropy: 1.4033 - accuracy: 0.5741\n",
      "Epoch 10/75\n",
      "25549/25549 [==============================] - 22s 861us/step - loss: 1.3927 - categorical_crossentropy: 1.3927 - accuracy: 0.5778\n",
      "Epoch 11/75\n",
      "25549/25549 [==============================] - 22s 877us/step - loss: 1.3836 - categorical_crossentropy: 1.3836 - accuracy: 0.5799\n",
      "Epoch 12/75\n",
      "25549/25549 [==============================] - 22s 868us/step - loss: 1.3765 - categorical_crossentropy: 1.3765 - accuracy: 0.5818\n",
      "Epoch 13/75\n",
      "25549/25549 [==============================] - 22s 877us/step - loss: 1.3692 - categorical_crossentropy: 1.3692 - accuracy: 0.5841\n",
      "Epoch 14/75\n",
      "25549/25549 [==============================] - 22s 869us/step - loss: 1.3637 - categorical_crossentropy: 1.3637 - accuracy: 0.5854\n",
      "Epoch 15/75\n",
      "25549/25549 [==============================] - 22s 875us/step - loss: 1.3583 - categorical_crossentropy: 1.3583 - accuracy: 0.5871\n",
      "Epoch 16/75\n",
      "25549/25549 [==============================] - 22s 878us/step - loss: 1.3538 - categorical_crossentropy: 1.3538 - accuracy: 0.5885\n",
      "Epoch 17/75\n",
      "25549/25549 [==============================] - 22s 879us/step - loss: 1.3494 - categorical_crossentropy: 1.3494 - accuracy: 0.5893\n",
      "Epoch 18/75\n",
      "25549/25549 [==============================] - 22s 879us/step - loss: 1.3458 - categorical_crossentropy: 1.3458 - accuracy: 0.5907\n",
      "Epoch 19/75\n",
      "25549/25549 [==============================] - 23s 884us/step - loss: 1.3423 - categorical_crossentropy: 1.3423 - accuracy: 0.5916\n",
      "Epoch 20/75\n",
      "25549/25549 [==============================] - 22s 880us/step - loss: 1.3389 - categorical_crossentropy: 1.3389 - accuracy: 0.5925\n",
      "Epoch 21/75\n",
      "25549/25549 [==============================] - 22s 872us/step - loss: 1.3358 - categorical_crossentropy: 1.3358 - accuracy: 0.5936\n",
      "Epoch 22/75\n",
      "25549/25549 [==============================] - 22s 871us/step - loss: 1.3336 - categorical_crossentropy: 1.3336 - accuracy: 0.5937\n",
      "Epoch 23/75\n",
      "25549/25549 [==============================] - 22s 870us/step - loss: 1.3311 - categorical_crossentropy: 1.3311 - accuracy: 0.5947\n",
      "Epoch 24/75\n",
      "25549/25549 [==============================] - 23s 882us/step - loss: 1.3291 - categorical_crossentropy: 1.3291 - accuracy: 0.5948\n",
      "Epoch 25/75\n",
      "25549/25549 [==============================] - 22s 877us/step - loss: 1.3267 - categorical_crossentropy: 1.3267 - accuracy: 0.5957\n",
      "Epoch 26/75\n",
      "25549/25549 [==============================] - 22s 873us/step - loss: 1.3245 - categorical_crossentropy: 1.3245 - accuracy: 0.5964\n",
      "Epoch 27/75\n",
      "25549/25549 [==============================] - 22s 879us/step - loss: 1.3227 - categorical_crossentropy: 1.3227 - accuracy: 0.5969\n",
      "Epoch 28/75\n",
      "25549/25549 [==============================] - 22s 872us/step - loss: 1.3211 - categorical_crossentropy: 1.3211 - accuracy: 0.5971\n",
      "Epoch 29/75\n",
      "25549/25549 [==============================] - 20s 799us/step - loss: 1.3195 - categorical_crossentropy: 1.3195 - accuracy: 0.5978\n",
      "Epoch 30/75\n",
      "25549/25549 [==============================] - 22s 877us/step - loss: 1.3177 - categorical_crossentropy: 1.3177 - accuracy: 0.5981\n",
      "Epoch 31/75\n",
      "25549/25549 [==============================] - 22s 878us/step - loss: 1.3156 - categorical_crossentropy: 1.3156 - accuracy: 0.5987\n",
      "Epoch 32/75\n",
      "25549/25549 [==============================] - 22s 878us/step - loss: 1.3145 - categorical_crossentropy: 1.3145 - accuracy: 0.5994\n",
      "Epoch 33/75\n",
      "25549/25549 [==============================] - 23s 891us/step - loss: 1.3136 - categorical_crossentropy: 1.3136 - accuracy: 0.5994\n",
      "Epoch 34/75\n",
      "25549/25549 [==============================] - 22s 871us/step - loss: 1.3119 - categorical_crossentropy: 1.3119 - accuracy: 0.5997\n",
      "Epoch 35/75\n",
      "25549/25549 [==============================] - 22s 876us/step - loss: 1.3108 - categorical_crossentropy: 1.3108 - accuracy: 0.6003\n",
      "Epoch 36/75\n",
      "25549/25549 [==============================] - 22s 872us/step - loss: 1.3099 - categorical_crossentropy: 1.3099 - accuracy: 0.6001\n",
      "Epoch 37/75\n",
      "25549/25549 [==============================] - 22s 876us/step - loss: 1.3090 - categorical_crossentropy: 1.3090 - accuracy: 0.6003\n",
      "Epoch 38/75\n",
      "25549/25549 [==============================] - 23s 883us/step - loss: 1.3076 - categorical_crossentropy: 1.3076 - accuracy: 0.6012\n",
      "Epoch 39/75\n",
      "25549/25549 [==============================] - 22s 876us/step - loss: 1.3069 - categorical_crossentropy: 1.3069 - accuracy: 0.6011\n",
      "Epoch 40/75\n",
      "25549/25549 [==============================] - 22s 880us/step - loss: 1.3059 - categorical_crossentropy: 1.3059 - accuracy: 0.6013\n",
      "Epoch 41/75\n",
      "25549/25549 [==============================] - 23s 883us/step - loss: 1.3045 - categorical_crossentropy: 1.3045 - accuracy: 0.6022\n",
      "Epoch 42/75\n",
      "25549/25549 [==============================] - 22s 876us/step - loss: 1.3040 - categorical_crossentropy: 1.3040 - accuracy: 0.6018\n",
      "Epoch 43/75\n",
      "25549/25549 [==============================] - 22s 881us/step - loss: 1.3029 - categorical_crossentropy: 1.3029 - accuracy: 0.6018\n",
      "Epoch 44/75\n",
      "25549/25549 [==============================] - 23s 886us/step - loss: 1.3026 - categorical_crossentropy: 1.3026 - accuracy: 0.6018\n",
      "Epoch 45/75\n",
      "25549/25549 [==============================] - 22s 879us/step - loss: 1.3014 - categorical_crossentropy: 1.3014 - accuracy: 0.6025\n",
      "Epoch 46/75\n",
      "25549/25549 [==============================] - 23s 882us/step - loss: 1.3004 - categorical_crossentropy: 1.3004 - accuracy: 0.6030\n",
      "Epoch 47/75\n",
      "25549/25549 [==============================] - 23s 882us/step - loss: 1.3000 - categorical_crossentropy: 1.3000 - accuracy: 0.6028\n",
      "Epoch 48/75\n",
      "25549/25549 [==============================] - 22s 876us/step - loss: 1.2988 - categorical_crossentropy: 1.2988 - accuracy: 0.6034\n",
      "Epoch 49/75\n",
      "25549/25549 [==============================] - 23s 886us/step - loss: 1.2989 - categorical_crossentropy: 1.2989 - accuracy: 0.6032\n",
      "Epoch 50/75\n",
      "25549/25549 [==============================] - 22s 869us/step - loss: 1.2980 - categorical_crossentropy: 1.2980 - accuracy: 0.6036\n",
      "Epoch 51/75\n",
      "25549/25549 [==============================] - 23s 890us/step - loss: 1.2975 - categorical_crossentropy: 1.2975 - accuracy: 0.6041\n",
      "Epoch 52/75\n",
      "25549/25549 [==============================] - 23s 891us/step - loss: 1.2971 - categorical_crossentropy: 1.2971 - accuracy: 0.6037\n",
      "Epoch 53/75\n",
      "25549/25549 [==============================] - 23s 891us/step - loss: 1.2966 - categorical_crossentropy: 1.2966 - accuracy: 0.6038\n",
      "Epoch 54/75\n",
      "25549/25549 [==============================] - 23s 897us/step - loss: 1.2956 - categorical_crossentropy: 1.2956 - accuracy: 0.6043\n",
      "Epoch 55/75\n",
      "25549/25549 [==============================] - 23s 893us/step - loss: 1.2948 - categorical_crossentropy: 1.2948 - accuracy: 0.6045\n",
      "Epoch 56/75\n",
      "25549/25549 [==============================] - 23s 885us/step - loss: 1.2946 - categorical_crossentropy: 1.2946 - accuracy: 0.6045\n",
      "Epoch 57/75\n",
      "25549/25549 [==============================] - 23s 890us/step - loss: 1.2945 - categorical_crossentropy: 1.2945 - accuracy: 0.6047\n",
      "Epoch 58/75\n",
      "25549/25549 [==============================] - 22s 870us/step - loss: 1.2931 - categorical_crossentropy: 1.2931 - accuracy: 0.6049\n",
      "Epoch 59/75\n",
      "25549/25549 [==============================] - 22s 878us/step - loss: 1.2928 - categorical_crossentropy: 1.2928 - accuracy: 0.6050\n",
      "Epoch 60/75\n",
      "25549/25549 [==============================] - 23s 917us/step - loss: 1.2920 - categorical_crossentropy: 1.2920 - accuracy: 0.6053\n",
      "Epoch 61/75\n",
      "25549/25549 [==============================] - 23s 893us/step - loss: 1.2920 - categorical_crossentropy: 1.2920 - accuracy: 0.6052\n",
      "Epoch 62/75\n",
      "25549/25549 [==============================] - 22s 878us/step - loss: 1.2917 - categorical_crossentropy: 1.2917 - accuracy: 0.6054\n",
      "Epoch 63/75\n",
      "25549/25549 [==============================] - 23s 903us/step - loss: 1.2911 - categorical_crossentropy: 1.2911 - accuracy: 0.6051\n",
      "Epoch 64/75\n",
      "25549/25549 [==============================] - 23s 894us/step - loss: 1.2905 - categorical_crossentropy: 1.2905 - accuracy: 0.6056\n",
      "Epoch 65/75\n",
      "25549/25549 [==============================] - 23s 891us/step - loss: 1.2900 - categorical_crossentropy: 1.2900 - accuracy: 0.6056\n",
      "Epoch 66/75\n",
      "25549/25549 [==============================] - 22s 865us/step - loss: 1.2892 - categorical_crossentropy: 1.2892 - accuracy: 0.6062\n",
      "Epoch 67/75\n",
      "25549/25549 [==============================] - 23s 885us/step - loss: 1.2890 - categorical_crossentropy: 1.2890 - accuracy: 0.6060\n",
      "Epoch 68/75\n",
      "25549/25549 [==============================] - 23s 898us/step - loss: 1.2890 - categorical_crossentropy: 1.2890 - accuracy: 0.6062\n",
      "Epoch 69/75\n",
      "25549/25549 [==============================] - 23s 897us/step - loss: 1.2885 - categorical_crossentropy: 1.2885 - accuracy: 0.6065\n",
      "Epoch 70/75\n",
      "25549/25549 [==============================] - 23s 899us/step - loss: 1.2878 - categorical_crossentropy: 1.2878 - accuracy: 0.6065\n",
      "Epoch 71/75\n",
      "25549/25549 [==============================] - 23s 896us/step - loss: 1.2880 - categorical_crossentropy: 1.2880 - accuracy: 0.6062\n",
      "Epoch 72/75\n",
      "25549/25549 [==============================] - 23s 887us/step - loss: 1.2873 - categorical_crossentropy: 1.2873 - accuracy: 0.6063\n",
      "Epoch 73/75\n",
      "25549/25549 [==============================] - 22s 869us/step - loss: 1.2872 - categorical_crossentropy: 1.2872 - accuracy: 0.6061\n",
      "Epoch 74/75\n",
      "25549/25549 [==============================] - 22s 859us/step - loss: 1.2865 - categorical_crossentropy: 1.2865 - accuracy: 0.6064\n",
      "Epoch 75/75\n",
      "25549/25549 [==============================] - 22s 875us/step - loss: 1.2860 - categorical_crossentropy: 1.2860 - accuracy: 0.6070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x143def048>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "seqlen = 40\n",
    "step = seqlen\n",
    "sentences = []\n",
    "for i in range(0, len(text) - seqlen - 1, step):\n",
    "    sentences.append(text[i: i + seqlen + 1])\n",
    "\n",
    "x = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, (char_in, char_out) in enumerate(zip(sentence[:-1], sentence[1:])):\n",
    "        x[i, t, char_indices[char_in]] = 1\n",
    "        y[i, t, char_indices[char_out]] = 1\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seqlen, len(chars)), return_sequences=True))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=0.01),\n",
    "    metrics=['categorical_crossentropy', 'accuracy']\n",
    ")\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Helper function to sample an index from a probability array.\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.exp(np.log(preds) / temperature)  # softmax\n",
    "    preds = preds / np.sum(preds)                #\n",
    "    probas = np.random.multinomial(1, preds, 1)  # sample index\n",
    "    return np.argmax(probas)                     #\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    \"\"\"Function invoked at end of each epoch. Prints generated text.\"\"\"\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - seqlen - 1)\n",
    "    \n",
    "    for diversity in [0.5]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + seqlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(200):\n",
    "            x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "            preds = model.predict(x_pred, verbose=0)\n",
    "            next_index = sample(preds[0, -1], diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=75,\n",
    "          callbacks=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Rap Lyric Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Generating lyrics\n",
      "... With diversity value: 0.6\n",
      "... Generating with seed: \"Like a shepherd having sex with a sheep,\"\n",
      "\n",
      "Like a shepherd having sex with a sheep, pussy is love\n",
      "Way too many fly with your mind, where the case I ain't nothin'\n",
      "They like when I was too much for my shit\n",
      "I was too bushed, you got a Pati, and the mall you with the star\n",
      "When you could talk, you gon' be all day with the ones\n",
      "I love it was pretendays my family shit\n",
      "I want you what you know where I chelf\n",
      "I'm just everybody believe it I stay toure on the club\n",
      "I ain't take a polis that I was learn and these nigga one bread for sure\n",
      "I'm talkin' about her doing mornin', this man with you\n",
      "And I never liked the club up (Yeah)\n",
      "\n",
      "[Verse 2: Travis Scott]\n",
      "I ain't lookin' for me way down\n",
      "For the schemin' shot as he askin' with you\n",
      "\n",
      "[Verse 2]\n",
      "I hope you know what's that she get it, I got to show me (Keep gon' go back)\n",
      "I see my waves all in\n",
      "All the way I want my team (row)\n",
      "Get it, get it, count it up, count it, count it\n",
      "I wanna was for me[Produced by Noah\" \"Drake Homies a standness)\n",
      "They want it to touch through the bean\n",
      "That a way that I would be on the face\n",
      "I heard what and I ain't know what they was hearin' in the check with some motion\n",
      "Only me hard with no are top, I want you is back and it ain't so I drivin' the same\n",
      "I know what to do is chances, he always with that new friends already, like the feelind last no more\n",
      "You want to love for a today (Yeah)\n",
      "Boy, ride for the time to get bad back (Ha)\n",
      "I just told me that you know what's here\n",
      "Count it all of the bitches when I mind\n",
      "I said you ain't take the fake no music and the done\n",
      "\n",
      "[Verse 1]\n",
      "Yeah, yeah!\n",
      "\n",
      "[Verse 2]\n",
      "Yeah, I don't tell my time\n",
      "You know how that should go\n",
      "They thinkin' 'bout brick on the hearts\n",
      "And the new face with they was a motherfuckers to was wait\n",
      "Well as someone next bottlom your kid\n",
      "Somethin' to say (Yeah! Been real)\n",
      "When I'm hurt on the counts\n",
      "Give you ain't no moats (Hey)\n",
      "I got shit to regult to me\n",
      "I know you wanna put the top\n",
      "It's for ya what you like home\n",
      "I had to do no real for her walls that you lost the same\n",
      "\n",
      "[Verse 1: J. Cole]\n",
      "I leave you and you count shit to the four going (Yeah!)\n",
      "You close\n",
      "I know what the used when I came in the doing to you\n",
      "Can I, I should be a lock of these never and want me\n",
      "And we could be ghost the stress, yeah me\n",
      "Last night shit is charge on my way\n",
      "And that's just how to tell you, oh\n",
      "\n",
      "[Verse 2]\n",
      "Too busy too lostor for the day we made it\n",
      "I want is it to the game, got my feet up\n",
      "He gone, mamacita\n",
      "(Woo)\n",
      "We got you, I can see myself was the thank and someone that shit with the pool (Bring)\n",
      "She said you know how that should go\n",
      "They said they heard we all don't want to do\n",
      "I went back in the clown was the side\n",
      "Sending with someone that I should be a hate and sleep and they can take it\n",
      "I feel like the new tonight now\n",
      "They know, yeah, you're a premental the rest\n",
      "So we better like I'm tryna lookin' up (Yeah)\n",
      "I had to feel a face to be the star, me you, you gone\n",
      "But that should go ass a million on the ain't the point shit, that's what have to do the best (Ayy)\n",
      "I went to put the things I know I was it, we underite\n",
      "Nah, natch, you should be schilin' up all some screaming\n",
      "And shit, I was the shit with the women tonight (Uh)\n",
      "\n",
      "[Verse 3: Drake]\n",
      "\n",
      "[Verse 2]\n",
      "Yeah, yeah, yeah, yeah\n",
      "\n",
      "[Chorus: Drake]\n",
      "I been part that now I love me\n",
      "I want all the world, I can see these niggas like a man in\n",
      "And I stop, that's what's when I wanted like a safe\n",
      "I can fuck a nigga like this shit to pop on that (doo's through the money)\n",
      "I ain't playin' with these neidin' actin' like I love\n",
      "Got a real shit that you can see my face\n",
      "I see the park my bills in the money\n",
      "Niggas see my mind, push my block and action\n",
      "So what a no go for your shit sandhing out to the say \"A dream)\n",
      "Five off that shit, gotta fuck the crew\n",
      "\n",
      "[Verse 1]\n",
      "When I count it all the way I'm too (Stop, it's one)\n",
      "\n",
      "[Chorus]\n",
      "You wanna be with me\n",
      "Then I had to dreams, look at my biggest like a man, that's my mama, mamacitu( on the pen up the motherfuckin' best (Ayy)\n",
      "Yeah, yeah, yeah, yeah, yeah\n",
      "I want a new fuckers to get to pushing it (Uh)\n",
      "On my way, if the fumous and back in my city\n",
      "Too much of these houses\n",
      "I ain't playin' it, can I was start like tripper (Boo-1-ouh)\n",
      "Fly the neck to you\n",
      "Only was a couple that reliaine we can stack in my soul\n",
      "And you want to call me in the morns\n",
      "When you so high with this feelin' her whip\n",
      "K.O.D., he hard as with your way, you love me for your heart\n",
      "And I stay and you lookin' at them living and with my main licken (Yeah)\n",
      "And that's what's happin' that into let me love me\n",
      "I said to me and the chast homies\n",
      "You know it's this shit is all that motherfucker\n",
      "Ain't no lakin' with my wood to my heart (Too bump, let's get drunk)\n",
      "Pasé is paint it to see your person that as deal, I said no worldder (Yeah, yeah)\n",
      "Scoulle, you ain't got me to the chest (Yeah)\n",
      "Started from the world for real\n",
      "I play it down so they told me (Working), when I'm 'bout to say that I ain't still up\n",
      "And I could fuck to a couple to the world\n",
      "My niggas is back in the bottom, now my lil' breathin' niggas in the music (Huh)\n",
      "I was trall from the bank when you can pauked\n",
      "I know my son for me, boy (Slop)\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('... Generating lyrics')\n",
    "diversity = .6\n",
    "print('... With diversity value:', diversity)\n",
    "\n",
    "generated = ''\n",
    "sentence = 'Like a shepherd having sex with a sheep, fuck what you heard'[:40]\n",
    "generated += sentence\n",
    "print('... Generating with seed: \"' + sentence + '\"')\n",
    "print()\n",
    "sys.stdout.write(generated)\n",
    "\n",
    "for i in range(5000):\n",
    "    x_pred = np.zeros((1, seqlen, len(chars)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_indices[char]] = 1.\n",
    "    preds = model.predict(x_pred, verbose=0)\n",
    "    next_index = sample(preds[0, -1], diversity)\n",
    "    next_char = indices_char[next_index]\n",
    "\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Produced', 'by', 'Phonix', 'Beats', 'and', 'J', 'Cole', '\\n', '\\n', 'Verse', '1', '\\n', 'First', 'things', 'first', 'rest', 'in', 'peace', 'Uncle', 'Phil']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# number of words to generate with the n-gram models\n",
    "NUM_OF_ITS = 100\n",
    "\n",
    "regex = r\"[\\w']+|[\\n]\"\n",
    "document = []\n",
    "\n",
    "words = re.findall(regex, text)\n",
    "\n",
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-Gram Model\n",
    "\n",
    "Words are generated looking at one previous word. (Probability distribution over possible next words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preach preach preach \n",
      " All the woman that You treat it private room and didn't have you comfort me give a baby when I see from it was entering a price on the ride for you blame for you love you get thin \n",
      " Your homegirl that assume 'cause you \n",
      " Chorus Drake \n",
      " Had the bottom now now \n",
      " Said it a flower \n",
      " Pull them kids is my pops he \n",
      " Pastor reverend for your name hold on my Rockstar skinnies Yeah they down the set us \n",
      " I ain't out the rumors \n",
      " Bitches can't help it did\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "transitions = defaultdict(list)\n",
    "for prev, current in zip(words, words[1:]):\n",
    "    transitions[prev].append(current)\n",
    "\n",
    "def generate_using_bigrams():\n",
    "    current = '\\n'\n",
    "    result = []\n",
    "    count = 0\n",
    "    while True:\n",
    "        next_word_candidates = transitions[current]\n",
    "        current = random.choice(next_word_candidates)\n",
    "        result.append(current)\n",
    "        if count == NUM_OF_ITS: return \" \".join(result)\n",
    "        count += 1\n",
    "\n",
    "res = generate_using_bigrams()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-Gram Model\n",
    "\n",
    "Words are generated looking at two previous words. (Probability distribution over possible next words)\n",
    "This results in many lyrics being directly copied over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You know that you're over and she eat it \n",
      " Blank Blank \n",
      " Pop anything pop anything to buy a vest Ayy \n",
      " I'm findin' the more I just checked Checked checked \n",
      " Hate that I met you in the air run it \n",
      " And I m tried in a dark room \n",
      " \n",
      " Chorus \n",
      " I'm on fire and them leavin' \n",
      " In the parking lot Gonzales Park odor \n",
      " We wrapping up plastic actually \n",
      " I just shoot \n",
      " \n",
      " Chorus Drake \n",
      " Looking for things you gotta go \n",
      " It's just the motion yeah \n",
      " Redemption's on your mind\n"
     ]
    }
   ],
   "source": [
    "trigram_transitions = defaultdict(list)\n",
    "starts = []\n",
    "\n",
    "for prev, current, nxt in zip(words, words[1:], words[2:]):\n",
    "    if prev == '\\n':\n",
    "        starts.append(current)\n",
    "    trigram_transitions[(prev, current)].append(nxt)\n",
    "    \n",
    "def generate_using_trigrams():\n",
    "    current = random.choice(starts)\n",
    "    prev = '\\n'\n",
    "    result = [current]\n",
    "    count = 0\n",
    "    \n",
    "    while True:\n",
    "        next_word_candidates = trigram_transitions[(prev, current)]\n",
    "        next_word = random.choice(next_word_candidates)\n",
    "        \n",
    "        prev, current = current, next_word\n",
    "        result.append(current)\n",
    "        if count == NUM_OF_ITS: return ' '.join(result)\n",
    "        count += 1\n",
    "        \n",
    "print(generate_using_trigrams())\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
